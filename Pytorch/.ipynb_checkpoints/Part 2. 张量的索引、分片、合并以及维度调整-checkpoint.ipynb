{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2.张量的索引、分片、合并以及维度调整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;张量作为有序的序列，也是具备数值索引的功能，并且基本索引方法和Python原生的列表、NumPy中的数组基本一致，当然，所有不同的是，PyTorch中还定义了一种采用函数来进行索引的方式。      \n",
    "&emsp;&emsp;而作为PyTorch中基本数据类型，张量即具备了列表、数组的基本功能，同时还充当着向量、矩阵、甚至是数据框等重要数据结构，因此PyTorch中也设置了非常完备的张量合并与变换的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、张量的符号索引（即数据下标索引）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;张量也是有序序列，我们可以根据每个元素在系统内的顺序“编号”，来找出特定的元素，也就是索引。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.一维张量索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;一维张量的索引过程和Python原生对象类型的索引一致，<font color=red>基本格式遵循`[start: end: step]`，表示从start开始，到end之前，注意是左闭右开</font>，索引的基本要点回顾如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.arange(1, 11)\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=red>从左到右，下标从零开始</font>\n",
    "    - 注意输出是0维张量，而不是整数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color=red>注：张量索引出来的结果还是零维张量， 而不是单独的数。因为一维张量由零维张量构成。要转化成单独的数，需要使用item()方法。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 冒号分隔，表示对某个区域进行索引，也就是所谓的切片\n",
    "    - <font color=red>基本格式遵循`[start: end: step]`，最后一个数是表示间隔，表示从第strat+1个元素开始，到第end+1个元素之前，每隔step-1取一个元素</font>\n",
    "    - <font color=red>注意：冒号左包含右不包含</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[1: 8]             # 索引其中2-9号元素，并且左包含右不包含"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 第二个冒号，表示索引的间隔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 6, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[1: 8: 2]          # 索引其中2-9号元素，左包含右不包含，且隔2-1个数取一个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 冒号前后没有值，表示索引这个区域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  4,  6,  8, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[1: : 2]           # 从第二个元素开始索引，一直到结尾，并且隔2-1个数取一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5, 7])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[: 8: 2]           # 从第一个元素开始索引到第9个元素（不包含），并且隔2-1个数取一个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color=red>在Python原生数据类型中，step可以小于0，从而实现从后往前取</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(range(1,11))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 9, 8, 7, 6, 5, 4, 3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[9: 1: -1] #从第9+1个元素开始，到第1+1个元素之前，间隔1-1个元素取值，因此不包含原列表中的第1个和第2个元素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color=red>在张量的索引中，step位必须大于0</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "step must be greater than zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b82bb967c8e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: step must be greater than zero"
     ]
    }
   ],
   "source": [
    "t1[9: 1: -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.二维张量索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;二维张量的索引逻辑和一维张量的索引逻辑基本相同，二维张量可以视为两个一维张量组合而成，而在实际的索引过程中，需要用逗号进行分隔，分别表示对哪个一维张量进行索引、以及具体的一维张量的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用reshape创建二维张量\n",
    "t2 = torch.arange(1, 10).reshape(3, 3)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2[0, 1]                  # 表示索引第0+1行、第1+1个（第1+1列的）元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2[0, ::2]                # 表示索引第一行、每隔2-1个元素取一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2[0, [0, 2]]             # 索引结果同上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3],\n",
       "        [7, 9]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2[::2, ::2]              # 表示每隔两行取一行、并且每一行中每隔两个元素取一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2[[0, 2], 1]              # 索引第0+1行第1+1列和第2+1行第1+1列的元素（应该是两个元素）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**理解**：对二维张量来说，基本可以视为是对矩阵的索引，并且行、列的索引遵照相同的索引规范，并用逗号进行分隔。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.三维张量的索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在二维张量索引的基础上，三维张量拥有三个索引的维度。<font color=red>我们将三维张量视作矩阵组成的序列，则在实际索引过程中拥有三个维度，分别是索引矩阵、索引矩阵的行、索引矩阵的列。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6],\n",
       "         [ 7,  8,  9]],\n",
       "\n",
       "        [[10, 11, 12],\n",
       "         [13, 14, 15],\n",
       "         [16, 17, 18]],\n",
       "\n",
       "        [[19, 20, 21],\n",
       "         [22, 23, 24],\n",
       "         [25, 26, 27]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = torch.arange(1, 28).reshape(3, 3, 3)\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3[1, 1, 1]          # 索引第1+1个矩阵中，第1+1行、第1+1个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 12],\n",
       "        [16, 18]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3[1, ::2, ::2]      # 索引第1+1个矩阵，行和列都是每隔2-1个取一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  3],\n",
       "         [ 7,  9]],\n",
       "\n",
       "        [[19, 21],\n",
       "         [25, 27]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3[:: 2, :: 2, :: 2]      # 每隔2-1个取一个矩阵，对于每个矩阵来说，行和列都是每隔2-1个取一个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**理解**：更为本质的角度去理解高维张量的索引，<font color=red>其实就是围绕张量的“形状”进行索引</font>\n",
    "- 第一个索引，对应在第一个维度上挑选\n",
    "- 第二个索引，对应在第二个维度上挑选\n",
    "- 以此类推"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3[1, 1, 1]             # 与shape一一对应"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、张量的函数索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在PyTorch中，我们还可以使用index_select函数，通过指定index来对张量进行索引。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.index_select(input, dim, index, *, out=None) → Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dim参数表示按照第dim+1个维度进行索引，比如dim=0表示按照第1个维度索引\n",
    "- index表示索引位置，如果是张量，则索引位置就是张量中的每一个元素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 一维张量的函数索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.tensor([1, 2])\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[1: 3]    # 切片法索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[[1, 2]]  # 列表法索引，引出的是第1+1和第2+1个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(t1, 0, indices) # 函数索引，引出的是第1+1和第2+1个元素 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在index_select函数中，第二个参数实际上代表的是索引的维度。对于t1这个一维向量来说，由于只有一个维度，因此第二个参数取值为0，就代表在第一个维度上进行索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 高维张量的函数索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数索引在很多模型中应用，因为在高维张量中更加清晰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.arange(12).reshape(4, 3)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.arange(1, 3)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(t2, 0, indices)   #在第0+1个维度上取出索引为1+1和2+1的部分  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim参数取值为0，代表在shape的第0+1个维度上索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2],\n",
       "        [ 4,  5],\n",
       "        [ 7,  8],\n",
       "        [10, 11]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(t2, 1, indices)  #在第1+1个维度上取出索引为1+1和2+1的部分  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim参数取值为1，代表在shape的第1+1个维度上索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是一个案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 18 19  0 24]\n",
      "torch.Size([4, 512, 28, 28])\n",
      "torch.Size([4, 5, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.arange(4*512*28*28).view(4,512,28,28)\n",
    "index=np.random.choice(32,5)# 在0到31内随机选5个值\n",
    "select=torch.index_select(a,1,index=torch.tensor(index,dtype=int))\n",
    "print(index)\n",
    "print(a.shape)\n",
    "print(select.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.index_select与直接索引的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29],\n",
      "         [30, 31, 32, 33, 34],\n",
      "         [35, 36, 37, 38, 39]]])\n",
      "tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19]])\n",
      "torch.Size([1, 4, 5])\n",
      "torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.arange(40).view(2,4,5)\n",
    "index=torch.tensor([1])\n",
    "select_1=torch.index_select(a,dim=0,index=index)\n",
    "select_2=a[0,:,:]\n",
    "print(select_1)\n",
    "print(select_2)\n",
    "print(select_1.shape) #函数索引后还是3维张量\n",
    "print(select_2.shape) #直接索引后变成2维张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 直接进行索引和利用torch.index_select索引最大的区别就在于：\n",
    "- <font color=red>直接进行索引数组维数会降低，利用torch.index_select索引数组维数不变</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、tensor.view()方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在正式介绍张量的切分方法之前，需要首先介绍PyTorch中的.view()方法。<font color=red>该方法会返回一个类似视图的结果，该结果和原张量对象共享一块数据存储空间，并且通过.view()方法，还可以改变对象结构，生成一个不同结构，但共享一个存储空间的张量。当然，共享一个存储空间，也就代表二者是“浅拷贝”的关系，修改其中一个，另一个也会同步进行更改。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(6).reshape(2, 3)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = t.view(3, 2)              # 构建一个数据相同，但形状不同的“视图”对象\n",
    "te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color=red>二维张量也可以用一维赋值方法，如下面只给了行索引，而未给列索引，因此把第1行的所有元素都赋值为1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0] = 1                       # 对t进行修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [1, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te                             # te同步变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>``tensor.view()``方法也可以用于升维或降维，但是注意view用于升维或降维时，不改变原张量的维度</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1],\n",
       "         [3, 4, 5]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = t.view(1, 2, 3)           # 维度也可以修改\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>“视图”的作用就是节省空间，而值得注意的是，在接下来介绍的很多切分张量的方法中，返回结果都是“视图”，而不是新生成一个对象。</font>\n",
    "- <font color=red>视图的思想来源于数据库</font>\n",
    "- <font color=red>视图使编程者可以改变、分片、合并张量，而不用占用新的空间，与基础数据类型如list等的分片不同，后者会生成新的对象，占用更多内存</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、张量的分片函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.分块：chunk函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;chunk函数能够按照某维度，对张量进行均匀切分，并且返回结果是原张量的视图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.arange(12).reshape(4, 3)\n",
    "t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.chunk(input, chunks, dim=0) → List of Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input (Tensor) – the tensor to split\n",
    "- chunks (int) – number of chunks to return，也就是要切成几块\n",
    "- dim (int) – dimension along which to split the tensor，也就是在哪个维度上切\n",
    "\n",
    "<font color=red>注意该函数返回的是一个元祖，其中的元素与原张量的维度相同，并没有产生降维</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2]]),\n",
       " tensor([[3, 4, 5]]),\n",
       " tensor([[6, 7, 8]]),\n",
       " tensor([[ 9, 10, 11]]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc = torch.chunk(t2, 4, dim=0)           # 在第零个维度上（按行），进行四等分\n",
    "tc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>注意：chunk返回结果是一个视图，不是新生成了一个对象，因此要时刻意识到分块结果的改变，会影响原张量</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 2]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc[0] # 返回元祖中的第一个值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 2])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc[0][0] # 取出二维张量中的第一行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc[0][0][0] = 1            # 修改tc中的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 2]]),\n",
       " tensor([[3, 4, 5]]),\n",
       " tensor([[6, 7, 8]]),\n",
       " tensor([[ 9, 10, 11]]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2                         # 原张量也会对应发生变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>当原张量不能均分时，chunk不会报错，而是会返回次一级均分的结果，即chunks-1等分，如果次一级还是不能均分，则继续向下找次一级等分，如果直到chunks=2也无法等分时，就会出现不均匀分片的结果，如原张量某一维度上是5，如果chunks设为4，则会生成3块，对应维度分别是2、2、1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 2],\n",
       "         [3, 4, 5]]),\n",
       " tensor([[ 6,  7,  8],\n",
       "         [ 9, 10, 11]]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(t2, 3, dim=0)            # 次一级均分结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch.chunk(t2, 3, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 2]]),\n",
       " tensor([[3, 4, 5]]),\n",
       " tensor([[6, 7, 8]]),\n",
       " tensor([[ 9, 10, 11]]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(t2, 5, dim=0)            # 次一级均分结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进阶用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0254,  0.4216,  1.7971,  0.6708, -1.0939],\n",
       "        [ 0.1823, -0.4759,  1.3940,  0.6531,  0.3483],\n",
       "        [ 0.2320,  1.5522,  0.5226, -1.7277,  0.5668],\n",
       "        [ 0.2708,  1.2064,  0.5778,  1.3170,  0.1301]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4,5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0254,  0.4216,  1.7971,  0.6708, -1.0939]]),\n",
       " tensor([[ 0.1823, -0.4759,  1.3940,  0.6531,  0.3483]]),\n",
       " tensor([[ 0.2320,  1.5522,  0.5226, -1.7277,  0.5668]]),\n",
       " tensor([[0.2708, 1.2064, 0.5778, 1.3170, 0.1301]]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.chunk(4,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``enumerate``函数返回index和对应元素，组成元祖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, tensor([[-0.0254,  0.4216,  1.7971,  0.6708, -1.0939]]))\n",
      "(1, tensor([[ 0.1823, -0.4759,  1.3940,  0.6531,  0.3483]]))\n",
      "(2, tensor([[ 0.2320,  1.5522,  0.5226, -1.7277,  0.5668]]))\n",
      "(3, tensor([[0.2708, 1.2064, 0.5778, 1.3170, 0.1301]]))\n"
     ]
    }
   ],
   "source": [
    "for data in enumerate(x.chunk(4,0)): # 在第一维度切分4块\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, tensor([[-0.0254,  0.4216],\n",
      "        [ 0.1823, -0.4759],\n",
      "        [ 0.2320,  1.5522],\n",
      "        [ 0.2708,  1.2064]]))\n",
      "(1, tensor([[ 1.7971,  0.6708],\n",
      "        [ 1.3940,  0.6531],\n",
      "        [ 0.5226, -1.7277],\n",
      "        [ 0.5778,  1.3170]]))\n",
      "(2, tensor([[-1.0939],\n",
      "        [ 0.3483],\n",
      "        [ 0.5668],\n",
      "        [ 0.1301]]))\n"
     ]
    }
   ],
   "source": [
    "for data in enumerate(x.chunk(4,1)): # 在第2维度切分4块\n",
    "    print(data)                      # chunk很有可能生成不均匀的块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.拆分：split函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font color=red>split既能进行均分，也能进行自定义切分。</font>当然，需要注意的是，和chunk函数一样，split返回结果也是view。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.split(tensor, split_size_or_sections, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensor (Tensor) – tensor to split.\n",
    "- split_size_or_sections (int) or (list(int)) – size of a single chunk or list of sizes for each chunk，输入一个数值时表示均分，传入列表则按照列表切分，但是要求列表元素之和等于原张量该维度上的数字\n",
    "- dim (int) – dimension along which to split the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.arange(12).reshape(4, 3)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [3, 4, 5]]),\n",
       " tensor([[ 6,  7,  8],\n",
       "         [ 9, 10, 11]]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(t2, 2, 0)           # 第二个参数只输入一个数值时表示均分，第三个参数表示切分的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2]]),\n",
       " tensor([[ 3,  4,  5],\n",
       "         [ 6,  7,  8],\n",
       "         [ 9, 10, 11]]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(t2, [1, 3], 0)           # 第二个参数输入一个序列时，表示按照序列数值进行切分，也就是1/3分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**，<font color=red>当第二个参数位输入一个序列时，序列的各数值的和必须等于对应维度下形状分量的取值。</font>例如，上述代码中，是按照第一个维度进行切分，而t2总共有4行，因此序列的求和必须等于4，也就是1+3=4，而序列中每个分量的取值，则代表切块大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2]]),\n",
       " tensor([[3, 4, 5]]),\n",
       " tensor([[6, 7, 8]]),\n",
       " tensor([[ 9, 10, 11]]))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(t2, [1, 1, 1, 1], 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2]]),\n",
       " tensor([[3, 4, 5]]),\n",
       " tensor([[ 6,  7,  8],\n",
       "         [ 9, 10, 11]]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(t2, [1, 1, 2], 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [3],\n",
       "         [6],\n",
       "         [9]]),\n",
       " tensor([[ 1,  2],\n",
       "         [ 4,  5],\n",
       "         [ 7,  8],\n",
       "         [10, 11]]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = torch.split(t2, [1, 2], 1) \n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts[0][0] # 取出第一个元祖的元素（是一个二维张量），然后取出二维张量的第一行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts[0][0] = 1                 # view进行修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1],\n",
       "         [3],\n",
       "         [6],\n",
       "         [9]]),\n",
       " tensor([[ 1,  2],\n",
       "         [ 4,  5],\n",
       "         [ 7,  8],\n",
       "         [10, 11]]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  2],\n",
       "        [ 3,  4,  5],\n",
       "        [ 6,  7,  8],\n",
       "        [ 9, 10, 11]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2                           # 原对象同步改变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color=red>tensor的split方法和array的split方法有很大的区别，array的split方法``array_split``是根据索引进行切分，只能进行等分。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.arange(9) \n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2]), array([3, 4]), array([5, 6]), array([7, 8])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_split(array, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、张量的合并操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;张量的合并操作类似与列表的追加元素，可以拼接、也可以堆叠。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=red>拼接函数：cat，返回结果为新张量，而不是视图</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.cat(tensors, dim=0, *, out=None) → Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensors (sequence of Tensors) – any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension.\n",
    "- dim (int, optional) – the dimension over which the tensors are concatenated\n",
    "- out (Tensor, optional) – the output tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### out为可选参数，与直接返回的结果相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(2, 3)\n",
    "b = torch.ones(2, 3)\n",
    "Y = torch.tensor([])\n",
    "X=torch.cat([a, b], out=Y)\n",
    "print(id(X)==id(Y))\n",
    "print(id(X.storage)==id(Y.storage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch中，可以使用cat函数实现张量的拼接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(2, 3)\n",
    "b = torch.ones(2, 3)\n",
    "c = torch.zeros(3, 3)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re = torch.cat([a, b])                  # 按照行进行拼接，dim默认取值为0\n",
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re[0][0]=5.0\n",
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a        # 说明合并后是新张量，而不是视图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, b], 1)               # 按照列进行拼接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在拼接时，两个张量除了待拼接维度外，如果其他维度上的值不同，则会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Got 2 and 3 in dimension 0 (The offending index is 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-8bdd1a857266>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# 形状不匹配时将报错\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Got 2 and 3 in dimension 0 (The offending index is 1)"
     ]
    }
   ],
   "source": [
    "torch.cat([a, c], 1)               # 形状不匹配时将报错"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意理解，拼接的本质是实现元素的堆积，也就是构成a、b两个二维张量的各一维张量的堆积，最终还是构成二维向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=red>堆叠函数：stack</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;和拼接不同，堆叠不是将元素拆分重装，而是简单的将各参与堆叠的对象分装到一个更高维度的张量里。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>堆叠和拼接的主要区别在于：堆叠会增加张量维度，而拼接不改变张量维度</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.]]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([a, b])                 # 堆叠之后，生成一个三维张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([a, b]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意对比二者区别，拼接之后维度不变，堆叠之后维度升高。拼接是把一个个元素单独提取出来之后再放到二维张量中，而堆叠则是直接将两个二维张量封装到一个三维张量中，因此，<font color=red>堆叠的要求更高，参与堆叠的张量必须shape形状完全相同。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, c])                # 横向拼接时，对行数没有一致性要求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [2, 3] at entry 0 and [3, 3] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-d0064288b164>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# 报错是因为a和c的shape不同\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [2, 3] at entry 0 and [3, 3] at entry 1"
     ]
    }
   ],
   "source": [
    "torch.stack([a, c])               # 报错是因为a和c的shape不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>在Python基础数据类型中，append()会把第二个list作为整体放入第一个list中，而extend()则会把第二个list完全打散，然后放在第一个list后面</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,3,5,7]\n",
    "b=[2,4,6,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, [2, 4, 6, 8]]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.append(b)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,3,5,7]\n",
    "b=[2,4,6,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.extend(b)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、张量维度变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此前我们介绍过，通过reshape方法，能够灵活调整张量的形状。而在实际操作张量进行计算时，往往需要另外进行降维和升维的操作，<font color=red>当我们需要除去不必要的维度时，可以使用squeeze函数，而需要手动升维时，则可采用unsqueeze函数。</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2 = a.reshape(1, 4)\n",
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(a2).ndim  # 重新降维至1维张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=red>squeeze函数：删除不必要的维度，也就是保留值不是1的维度</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.],\n",
       "          [0.],\n",
       "          [0.]]]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.zeros(1, 1, 3, 1)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 1])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t张量解释：一个包含一个三维的四维张量，三维张量只包含一个三行一列的二维张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(t).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转化后生成了一个一维张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 3, 2, 1, 2])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.zeros(1, 1, 3, 2, 1, 2)\n",
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(t1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单理解，squeeze就相当于提出了shape返回结果中的1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=red>unsqeeze函数：手动升维，目的是使用矩阵乘法（二维张量及以上），而不是用向量乘法（一维张量）</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 1, 2])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.zeros(1, 2, 1, 2)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方式是在dim处插入一个值为1的新维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0., 0.]],\n",
       "\n",
       "          [[0., 0.]]]]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(t, dim = 0)              # 在第1个维度索引上升高1个维度，不改变t本身的shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(t, dim = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 1, 1, 2])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(t, dim = 2).shape         # 在第3个维度索引上升高1个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 1, 2, 1])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(t, dim = 4).shape          # 在第5个维度索引上升高1个维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>注意理解维度和shape返回结果一一对应的关系，shape返回的序列有几个元素，张量就有多少维度。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>注意torch.squeeze和torch.unsqueeze都不改变原张量的shape，需要用一个新的变量来接收升维或降维的结果</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
